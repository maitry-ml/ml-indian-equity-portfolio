{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1J7RAP4m1n2_Keta3sJvZrae3Yiwg0Z8D",
      "authorship_tag": "ABX9TyMR/xS5VoZ0Qr4fv0H8tGNG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maitry-ml/ml-indian-equity-portfolio/blob/main/DSAI_FIN_ASSGN2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWWgxtrIHTGO"
      },
      "outputs": [],
      "source": [
        "pip install yfinance"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. MARKET DATA"
      ],
      "metadata": {
        "id": "igPlxMpngoeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "tickers = [\"RELIANCE.NS\", \"HDFCBANK.NS\", \"INFY.NS\",\n",
        "           \"BHARTIARTL.NS\", \"HINDUNILVR.NS\", \"M&M.NS\"]\n",
        "\n",
        "# Download all at once\n",
        "raw = yf.download(tickers, start=\"2020-01-01\", end=\"2025-12-31\", auto_adjust=True, group_by=\"ticker\")\n",
        "\n",
        "# Reshape to long format\n",
        "all_data = []\n",
        "for ticker in tickers:\n",
        "    df = raw[ticker].copy()\n",
        "    df = df.reset_index()\n",
        "    df[\"Stock\"] = ticker\n",
        "    df = df[[\"Date\", \"Stock\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
        "    all_data.append(df)\n",
        "\n",
        "panel = pd.concat(all_data, ignore_index=True)\n",
        "panel.columns.name = None\n",
        "\n",
        "# ── Count missing rows BEFORE dropping ──────────────────────────────\n",
        "missing_rows = panel[\"Close\"].isna().sum()\n",
        "total_rows   = len(panel)\n",
        "print(f\"Total rows        : {total_rows}\")\n",
        "print(f\"Rows with no data : {missing_rows}\")\n",
        "print(f\"Per stock breakdown:\")\n",
        "print(panel.groupby(\"Stock\")[\"Close\"].apply(lambda x: x.isna().sum()).rename(\"Missing Rows\"))\n",
        "print()\n",
        "\n",
        "# ── Now drop ─────────────────────────────────────────────────────────\n",
        "market_data = panel.dropna(subset=[\"Close\"])\n",
        "\n",
        "print(market_data.head(10))\n",
        "print(\"Shape after dropping:\", market_data.shape)"
      ],
      "metadata": {
        "id": "xuYo7f4VQVyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "market_data"
      ],
      "metadata": {
        "id": "7nwVI2n7VXqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. MACRO INDICATORS\n"
      ],
      "metadata": {
        "id": "Kr58cPDYYjRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ── STEP 2: Macro Indicators  ─────────────────────────────────\n",
        "import io\n",
        "import requests\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "START = \"2019-12-31\"\n",
        "END   = \"2025-12-31\"\n",
        "daily_idx = pd.date_range(START, END, freq=\"D\")\n",
        "\n",
        "# ── 2a. USD-INR and Crude Oil via yfinance ────────────────────────────\n",
        "macro_tickers = {\n",
        "    \"USDINR\"  : \"INR=X\",\n",
        "    \"CrudeOil\": \"CL=F\",\n",
        "}\n",
        "\n",
        "macro_raw = yf.download(\n",
        "    list(macro_tickers.values()),\n",
        "    start=START, end=END,\n",
        "    auto_adjust=True,\n",
        "    group_by=\"ticker\"\n",
        ")\n",
        "\n",
        "macro_frames = []\n",
        "for col_name, ticker in macro_tickers.items():\n",
        "    s = macro_raw[ticker][\"Close\"].copy()\n",
        "    s.name = col_name\n",
        "    macro_frames.append(s)\n",
        "\n",
        "macro_df = pd.concat(macro_frames, axis=1)\n",
        "macro_df.index = pd.to_datetime(macro_df.index)\n",
        "macro_df = macro_df.reindex(daily_idx).ffill()\n",
        "\n",
        "# ── 2b. India 10Y Bond Yield via FRED ───────\n",
        "fred_url = (\n",
        "    \"https://fred.stlouisfed.org/graph/fredgraph.csv\"\n",
        "    \"?id=INDIRLTLT01STM\"\n",
        ")\n",
        "try:\n",
        "\n",
        "    resp = requests.get(fred_url)\n",
        "    in10y = pd.read_csv(io.StringIO(resp.text),\n",
        "                        parse_dates=[\"observation_date\"],\n",
        "                        index_col=\"observation_date\")\n",
        "    in10y.columns = [\"IN10Y\"]\n",
        "    in10y = in10y.replace(\".\", float(\"nan\")).astype(float)\n",
        "    in10y = in10y.reindex(daily_idx).ffill()\n",
        "    macro_df[\"IN10Y\"] = in10y[\"IN10Y\"].values\n",
        "    print(\"India 10Y loaded from FRED\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️  FRED fetch failed: {e}\")\n",
        "\n",
        "# ── 2c. CPI (monthly → daily) ─────────────────────────────────────────\n",
        "cpi_data = {\n",
        "    \"2020-01\": 7.59, \"2020-02\": 6.58, \"2020-03\": 5.84, \"2020-04\": 7.22,\n",
        "    \"2020-05\": 5.84, \"2020-06\": 6.09, \"2020-07\": 6.93, \"2020-08\": 6.69,\n",
        "    \"2020-09\": 7.27, \"2020-10\": 7.61, \"2020-11\": 6.93, \"2020-12\": 4.59,\n",
        "    \"2021-01\": 4.06, \"2021-02\": 5.03, \"2021-03\": 5.52, \"2021-04\": 4.23,\n",
        "    \"2021-05\": 6.30, \"2021-06\": 6.26, \"2021-07\": 5.59, \"2021-08\": 5.30,\n",
        "    \"2021-09\": 4.35, \"2021-10\": 4.48, \"2021-11\": 4.91, \"2021-12\": 5.59,\n",
        "    \"2022-01\": 6.01, \"2022-02\": 6.07, \"2022-03\": 6.95, \"2022-04\": 7.79,\n",
        "    \"2022-05\": 7.04, \"2022-06\": 7.01, \"2022-07\": 6.71, \"2022-08\": 7.00,\n",
        "    \"2022-09\": 7.41, \"2022-10\": 6.77, \"2022-11\": 5.88, \"2022-12\": 5.72,\n",
        "    \"2023-01\": 6.52, \"2023-02\": 6.44, \"2023-03\": 5.66, \"2023-04\": 4.70,\n",
        "    \"2023-05\": 4.25, \"2023-06\": 4.81, \"2023-07\": 7.44, \"2023-08\": 6.83,\n",
        "    \"2023-09\": 5.02, \"2023-10\": 4.87, \"2023-11\": 5.55, \"2023-12\": 5.69,\n",
        "    \"2024-01\": 5.10, \"2024-02\": 5.09, \"2024-03\": 4.85, \"2024-04\": 4.83,\n",
        "    \"2024-05\": 4.75, \"2024-06\": 5.08, \"2024-07\": 3.54, \"2024-08\": 3.65,\n",
        "    \"2024-09\": 5.49, \"2024-10\": 6.21, \"2024-11\": 5.48, \"2024-12\": 5.22,\n",
        "    \"2025-01\": 4.26, \"2025-02\": 3.61, \"2025-03\": 3.34, \"2025-04\": 3.16,\n",
        "    \"2025-05\": 2.82, \"2025-06\": 2.10,\n",
        "}\n",
        "cpi_series = pd.Series(cpi_data)\n",
        "cpi_series.index = pd.to_datetime(cpi_series.index)\n",
        "cpi_daily = cpi_series.reindex(daily_idx).ffill()\n",
        "macro_df[\"CPI_YoY\"] = cpi_daily.values\n",
        "\n",
        "\n",
        "# ── 2d. Lagging everything by 1 day (no look-ahead bias) ──────────────────\n",
        "macro_lagged = macro_df.shift(1)\n",
        "macro_lagged.index.name = \"Date\"\n",
        "\n",
        "macro_lagged = macro_lagged.loc[\"2020-01-01\":]  # dropping 31st DEC\n",
        "\n",
        "print(\"\\nFinal macro shape:\", macro_lagged.shape)\n"
      ],
      "metadata": {
        "id": "sQFsY6xUYubj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, df in [(\"macro_df\", macro_df), (\"macro_lagged\", macro_lagged)]:\n",
        "    print(f\"{name}\")\n",
        "    print(df.isna().sum().to_frame(\"  Missing\"))\n",
        "    print(f\"Total: {df.isna().sum().sum()} / {df.size} cells\\n\")"
      ],
      "metadata": {
        "id": "fzyFeVzif-fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(macro_df.head())"
      ],
      "metadata": {
        "id": "5rR4l83ydkOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(macro_lagged.head())"
      ],
      "metadata": {
        "id": "Yl-_drS3dXIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. FUNDAMENTAL DATA"
      ],
      "metadata": {
        "id": "JGlqUx-NggAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def load_from_drive(file_id):\n",
        "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "    return pd.read_csv(url)\n",
        "\n",
        "\n",
        "\n",
        "eps_raw    = load_from_drive(\"1B1KvYlMUsFyUZeDhQGza759UWh7yUjY4\")\n",
        "ratios_raw = load_from_drive(\"12yY0yP97bWWJ1FQLV1zFGaU88-7fqZg5\")\n",
        "\n",
        "print(eps_raw.head())"
      ],
      "metadata": {
        "id": "b64OdDVJLcFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── STEP 3a. Align quarterly EPS → daily ─────────────────────────────\n",
        "all_eps = []\n",
        "\n",
        "daily_idx_extended = pd.date_range(\"2019-09-01\", \"2025-12-31\", freq=\"D\")\n",
        "\n",
        "for ticker in eps_raw[\"Stock\"].unique():\n",
        "    df = eps_raw[eps_raw[\"Stock\"] == ticker].copy()\n",
        "\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Quarter\"], format=\"%b %Y\") \\\n",
        "                   + pd.offsets.MonthEnd(0) \\\n",
        "                   + pd.DateOffset(days=45)\n",
        "\n",
        "    df = df.set_index(\"Date\")[[\"EPS\"]].sort_index()\n",
        "    df = pd.to_numeric(df[\"EPS\"], errors=\"coerce\").to_frame()\n",
        "    df = df.groupby(level=0).last()\n",
        "\n",
        "    eps_daily = df.reindex(daily_idx_extended).ffill().bfill()\n",
        "    eps_daily = eps_daily.loc[\"2020-01-01\":\"2025-12-31\"]\n",
        "    eps_daily[\"Stock\"] = ticker\n",
        "    all_eps.append(eps_daily)\n",
        "\n",
        "eps_panel = pd.concat(all_eps).reset_index()\n",
        "eps_panel.columns.name = None\n",
        "eps_panel.rename(columns={\"index\": \"Date\"}, inplace=True)\n",
        "print(\"✅ EPS aligned:\", eps_panel.shape)\n",
        "print(eps_panel.head())"
      ],
      "metadata": {
        "id": "7qHJF0N4Mgkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── 3b. Align annual ROE & D/E → daily ───────────────────────────────\n",
        "all_ratios = []\n",
        "daily_idx  = pd.date_range(\"2020-01-01\", \"2025-12-31\", freq=\"D\")\n",
        "\n",
        "for ticker in ratios_raw[\"Stock\"].unique():\n",
        "    df = ratios_raw[ratios_raw[\"Stock\"] == ticker].copy()\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Year\"].astype(str) + \"-03-31\")\n",
        "    df = df.set_index(\"Date\")[[\"ROE\",\"DebtEquity\"]].sort_index()\n",
        "    df = df.apply(pd.to_numeric, errors=\"coerce\")\n",
        "    df = df.groupby(level=0).last()\n",
        "\n",
        "    ratios_daily = df.reindex(daily_idx).ffill().bfill()\n",
        "    ratios_daily[\"Stock\"] = ticker\n",
        "    all_ratios.append(ratios_daily)\n",
        "\n",
        "ratios_panel = pd.concat(all_ratios).reset_index()\n",
        "ratios_panel.columns.name = None\n",
        "ratios_panel.rename(columns={\"index\": \"Date\"}, inplace=True)\n",
        "print(\"✅ ROE & D/E aligned:\", ratios_panel.shape)\n",
        "\n",
        "# ── 3c. Merge EPS + ROE + D/E ─────────────────────────────────────────\n",
        "fundamentals = pd.merge(eps_panel, ratios_panel, on=[\"Date\",\"Stock\"], how=\"left\")\n",
        "fundamentals.columns.name = None\n",
        "\n",
        "print(\"\\nShape\", fundamentals.shape)\n",
        "print(\"\\nSample\")\n",
        "print(fundamentals.head())\n"
      ],
      "metadata": {
        "id": "rn3hiOFHMzi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jan 2020 → Mar 2021 = 15 months out of 72 months total\n",
        "= about 21% of the dataset has approximated ROE & D/E"
      ],
      "metadata": {
        "id": "FSY7_kuRa9um"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. SENTIMENT DATA"
      ],
      "metadata": {
        "id": "TfBUFi5qQJA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Real-time sentiment is demonstrated using GNews API + FinBERT. Due to historical news API limitations,Sector based Nifty return was was used as sentiment proxy for 2020-2025 backtesting period. All sentiment features were lagged by 1 trading day to prevent look-ahead bias."
      ],
      "metadata": {
        "id": "cbNLYICTQODV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ── Installing required libraries ────────────────────────────────────────\n",
        "!pip install transformers torch gnews -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gnews import GNews\n",
        "from transformers import pipeline\n",
        "import time\n",
        "\n",
        "print(\"✅ Libraries installed\")"
      ],
      "metadata": {
        "id": "ZN61vOKiQMm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── Loading FinBERT model ────────────────────────────────────────────────\n",
        "print(\"Loading FinBERT... (takes 1-2 minutes first time)\")\n",
        "finbert = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"ProsusAI/finbert\",\n",
        "    return_all_scores=True\n",
        ")\n",
        "print(\"✅ FinBERT loaded\")"
      ],
      "metadata": {
        "id": "UMzLyMkuQtnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── Stock search terms ────────────────────────────────────────────────\n",
        "stock_queries = {\n",
        "    \"RELIANCE.NS\"   : \"Reliance Industries stock\",\n",
        "    \"HDFCBANK.NS\"   : \"HDFC Bank stock\",\n",
        "    \"INFY.NS\"       : \"Infosys stock\",\n",
        "    \"M&M.NS\"        : \"Mahindra Mahindra stock\",\n",
        "    \"BHARTIARTL.NS\" : \"Bharti Airtel stock\",\n",
        "    \"HINDUNILVR.NS\" : \"Hindustan Unilever HUL stock\"\n",
        "}"
      ],
      "metadata": {
        "id": "lafkunPVRH2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ──scoring function ────────────────────────────────────────────\n",
        "def get_finbert_score(headlines):\n",
        "    if not headlines:\n",
        "        return 0.0\n",
        "\n",
        "    scores = []\n",
        "    for headline in headlines:\n",
        "        try:\n",
        "            result = finbert(headline[:512])\n",
        "            # result = [{'label': 'positive', 'score': 0.75}]\n",
        "            label = result[0][\"label\"]\n",
        "            score = result[0][\"score\"]\n",
        "\n",
        "\n",
        "            if label == \"positive\":\n",
        "                net = score\n",
        "            elif label == \"negative\":\n",
        "                net = -score\n",
        "            else:  # neutral\n",
        "                net = 0.0\n",
        "\n",
        "            scores.append(net)\n",
        "        except Exception as e:\n",
        "            print(f\"  ⚠️ Skipping: {e}\")\n",
        "            continue\n",
        "\n",
        "    return round(float(np.mean(scores)), 4) if scores else 0.0"
      ],
      "metadata": {
        "id": "VVUJh5u1RW5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── fetching news WITH dates ────────────────────────────────────\n",
        "real_sentiment = []\n",
        "\n",
        "for ticker, query in stock_queries.items():\n",
        "    print(f\"\\nFetching news for {ticker}...\")\n",
        "\n",
        "    try:\n",
        "        articles = google_news.get_news(query)\n",
        "\n",
        "        if not articles:\n",
        "            print(f\"  No news found for {ticker}\")\n",
        "            continue\n",
        "\n",
        "        # ── Extract headline AND date per article ─────────────────────\n",
        "        dated_headlines = []\n",
        "        for article in articles:\n",
        "            try:\n",
        "                title     = article[\"title\"]\n",
        "                pub_date  = pd.to_datetime(article[\"published date\"]).normalize()\n",
        "                dated_headlines.append((pub_date, title))\n",
        "                print(f\"  [{pub_date.date()}] {title[:60]}...\")\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # ── Grouping headlines by date ───────────────────────────────────\n",
        "        from collections import defaultdict\n",
        "        date_groups = defaultdict(list)\n",
        "        for pub_date, title in dated_headlines:\n",
        "            date_groups[pub_date].append(title)\n",
        "\n",
        "        # ── Scoring each date separately ────────────────────────────────\n",
        "        for pub_date, headlines in date_groups.items():\n",
        "            score = get_finbert_score(headlines)\n",
        "            real_sentiment.append({\n",
        "                \"Date\"      : pub_date,\n",
        "                \"Stock\"     : ticker,\n",
        "                \"Sentiment\" : score,\n",
        "                \"Headlines\" : len(headlines)\n",
        "            })\n",
        "            print(f\"  {pub_date.date()} → {len(headlines)} headlines → score: {score}\")\n",
        "\n",
        "        time.sleep(2)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  Error: {e}\")\n",
        "\n",
        "real_sentiment_df = pd.DataFrame(real_sentiment)\n",
        "real_sentiment_df = real_sentiment_df.sort_values([\"Stock\",\"Date\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "RHyDSSzHRcAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_sentiment_df = real_sentiment_df.sort_values([\"Stock\",\"Date\"]).reset_index(drop=True)\n",
        "print(real_sentiment_df)"
      ],
      "metadata": {
        "id": "QN5EKxMzV2AU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Proxy sentiment for Historical Data**"
      ],
      "metadata": {
        "id": "vyMKgH2cWWBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Due to historical news API limitations, Nifty50 return-based sentiment proxy is used for 2020-2025 backtesting period. All sentiment features are lagged by 1 trading day to prevent look-ahead bias."
      ],
      "metadata": {
        "id": "l4e0WLZrWRru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ── Stock-Specific Sentiment using Sector Indices ─────────────────────\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "daily_idx = pd.date_range(\"2020-01-01\", \"2025-12-31\", freq=\"D\")\n",
        "\n",
        "# ── Sector index for each stock ───────────────────────────────────────\n",
        "sector_indices = {\n",
        "    \"RELIANCE.NS\"   : \"^CNXENERGY\",\n",
        "    \"HDFCBANK.NS\"   : \"^NSEBANK\",\n",
        "    \"INFY.NS\"       : \"^CNXIT\",\n",
        "    \"M&M.NS\"        : \"^CNXAUTO\",\n",
        "    \"BHARTIARTL.NS\" : \"^CNXMEDIA\",\n",
        "    \"HINDUNILVR.NS\" : \"^CNXFMCG\"\n",
        "}\n",
        "\n",
        "def compute_sentiment(ticker):\n",
        "    \"\"\"Fetching sector index and converting returns to sentiment score.\"\"\"\n",
        "    try:\n",
        "        data = yf.download(ticker, start=\"2019-12-31\",\n",
        "                           end=\"2025-12-31\", auto_adjust=True)\n",
        "        data.columns = data.columns.get_level_values(0)\n",
        "\n",
        "        # Calculating returns on trading days only\n",
        "        returns = data[\"Close\"].pct_change().dropna()\n",
        "\n",
        "        # Clipping and normalize to -1 to +1\n",
        "        std     = float(returns.std())\n",
        "        clipped = returns.clip(-3*std, 3*std)\n",
        "        proxy   = (clipped / (3*std)).fillna(0)\n",
        "\n",
        "        # Reindexing to calendar days and forward fill weekends/holidays\n",
        "        proxy.index = pd.to_datetime(proxy.index)\n",
        "        proxy = proxy.reindex(daily_idx).ffill().fillna(0)\n",
        "\n",
        "        return proxy\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Failed for {ticker}: {e} \")\n",
        "        return None\n",
        "\n",
        "\n",
        "# ── Computing sentiment per stock ───────────────────────────────────────\n",
        "proxy_frames = []\n",
        "\n",
        "for stock_ticker, sector_ticker in sector_indices.items():\n",
        "    print(f\"\\nFetching sector index {sector_ticker} for {stock_ticker}...\")\n",
        "\n",
        "    proxy = compute_sentiment(sector_ticker)\n",
        "\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"Date\"      : daily_idx,\n",
        "        \"Stock\"     : stock_ticker,\n",
        "        \"Sentiment\" : proxy.values\n",
        "    })\n",
        "    proxy_frames.append(df)\n",
        "\n",
        "\n",
        "sentiment_panel = pd.concat(proxy_frames, ignore_index=True)\n",
        "\n",
        "# ── Lag by 1 day per stock ────────────────────────────────────────────\n",
        "sentiment_panel = sentiment_panel.sort_values([\"Stock\",\"Date\"])\n",
        "sentiment_panel[\"Sentiment\"] = sentiment_panel.groupby(\"Stock\")[\"Sentiment\"].shift(1)\n",
        "sentiment_panel = sentiment_panel.dropna(subset=[\"Sentiment\"])\n",
        "sentiment_panel[\"Date\"] = pd.to_datetime(sentiment_panel[\"Date\"])\n",
        "sentiment_panel = sentiment_panel.reset_index(drop=True)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UFt7DnB29LzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── Verify different scores per stock ────────────────────────────────\n",
        "print(\"\\nSector-Specific Sentiment\")\n",
        "print(\"\\nShape:\", sentiment_panel.shape)\n",
        "print(\"\\nSame date different stocks should have DIFFERENT values:\")\n",
        "print(sentiment_panel[sentiment_panel[\"Date\"] == \"2020-01-03\"][[\"Date\",\"Stock\",\"Sentiment\"]])\n",
        "print(\"\\nSentiment range:\")\n",
        "print(sentiment_panel.groupby(\"Stock\")[\"Sentiment\"].describe())"
      ],
      "metadata": {
        "id": "ERiQxnsOBjKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0TXl3o_9BtQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_csv = sentiment_panel[[\"Date\", \"Stock\", \"Sentiment\"]].copy()\n",
        "sentiment_csv = sentiment_csv.sort_values([\"Stock\", \"Date\"]).reset_index(drop=True)\n",
        "\n",
        "print(\"=== Sentiment CSV Sample ===\")\n",
        "print(sentiment_csv.head(10))\n",
        "print(\"\\nShape:\", sentiment_csv.shape)\n",
        "\n",
        "# Save to CSV\n",
        "sentiment_csv.to_csv(\"sentiment_scores.csv\", index=False)\n",
        "print(\"✅ Saved as sentiment_scores.csv\")"
      ],
      "metadata": {
        "id": "2P-JXtUiCDoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"sentiment_scores.csv\")\n"
      ],
      "metadata": {
        "id": "1Ow6D14WCGWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Feature Engineering"
      ],
      "metadata": {
        "id": "gCIB62biJCXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ── STEP 5: Feature Engineering ───────────────────────────────────────\n",
        "!pip install ta -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ta\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# ── 5a. Start with market OHLCV panel ────────────────────────────────\n",
        "# 'market_data' is  OHLCV dataframe from Step 1\n",
        "# Make sure it has: Date, Stock, Open, High, Low, Close, Volume\n",
        "print(\"Market panel shape:\", market_data.shape)\n",
        "print(panel.head())"
      ],
      "metadata": {
        "id": "zhZoPU4iJOJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── STEP 5b: Essential Features Only ─────────────────────────────────\n",
        "feature_frames = []\n",
        "\n",
        "for ticker in market_data[\"Stock\"].unique():\n",
        "    print(f\"Processing {ticker}...\")\n",
        "    df = market_data[market_data[\"Stock\"] == ticker].copy()\n",
        "    df = df.sort_values(\"Date\").reset_index(drop=True)\n",
        "\n",
        "    # ── Log Return ────────────────────────────────────────────────────\n",
        "    df[\"LogReturn\"]   = np.log(df[\"Close\"] / df[\"Close\"].shift(1))\n",
        "\n",
        "    # ── Lagged Return ─────────────────────────────────────────────────\n",
        "    df[\"Return_lag1\"] = df[\"LogReturn\"].shift(1)\n",
        "\n",
        "    # ── RSI ───────────────────────────────────────────────────────────\n",
        "    df[\"RSI14\"]       = ta.momentum.RSIIndicator(\n",
        "                            close=df[\"Close\"], window=14).rsi()\n",
        "\n",
        "    # ── Price to SMA20 ratio ──────────────────────────────────────────\n",
        "    df[\"SMA20\"]             = df[\"Close\"].rolling(20).mean()\n",
        "    df[\"Price_SMA20_ratio\"] = df[\"Close\"] / df[\"SMA20\"]\n",
        "\n",
        "    # ── Volatility ────────────────────────────────────────────────────\n",
        "    df[\"Volatility20\"] = df[\"LogReturn\"].rolling(20).std()\n",
        "\n",
        "    # ── Volume change ────────────────────────────────────────────────\n",
        "    df[\"LogVolume\"] = np.log(df[\"Volume\"].replace(0, np.nan) / df[\"Volume\"].replace(0, np.nan).shift(1)\n",
        ")\n",
        "\n",
        "    # ── Target: next day log return ───────────────────────────────────\n",
        "    df[\"Target\"]       = df[\"LogReturn\"].shift(-1)\n",
        "\n",
        "    # Drop SMA20 raw\n",
        "    df = df.drop(columns=[\"SMA20\"])\n",
        "\n",
        "    feature_frames.append(df)\n",
        "\n",
        "features = pd.concat(feature_frames, ignore_index=True)\n",
        "print(\"\\n✅ Technical features shape:\", features.shape)\n",
        "print(features.columns.tolist())"
      ],
      "metadata": {
        "id": "v7TDAbQUMWdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iK0BWCSc_RO-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MERGING"
      ],
      "metadata": {
        "id": "8F-RTP3FNsVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ── 5c. Merge all data sources ────────────────────────────────────────\n",
        "features[\"Date\"]        = pd.to_datetime(features[\"Date\"])\n",
        "macro_lagged.index      = pd.to_datetime(macro_lagged.index)\n",
        "fundamentals[\"Date\"]    = pd.to_datetime(fundamentals[\"Date\"])\n",
        "sentiment_panel[\"Date\"] = pd.to_datetime(sentiment_panel[\"Date\"])\n",
        "\n",
        "# Reset macro index to merge on Date\n",
        "macro_reset = macro_lagged.reset_index()\n",
        "\n",
        "# ── Merge step by step ────────────────────────────────────────────────\n",
        "master = features.copy()\n",
        "\n",
        "# Merge macro (on Date only — same for all stocks)\n",
        "master = pd.merge(master, macro_reset,\n",
        "                  on=\"Date\", how=\"left\")\n",
        "\n",
        "# Merge fundamentals (on Date + Stock)\n",
        "master = pd.merge(master, fundamentals[[\"Date\",\"Stock\",\"EPS\",\"ROE\",\"DebtEquity\"]],\n",
        "                  on=[\"Date\",\"Stock\"], how=\"left\")\n",
        "\n",
        "# Merge sentiment (on Date + Stock)\n",
        "master = pd.merge(master, sentiment_panel[[\"Date\",\"Stock\",\"Sentiment\"]],\n",
        "                  on=[\"Date\",\"Stock\"], how=\"left\")\n",
        "\n",
        "# ── Calculate P/E ratio ───────────────────────────────────────────────\n",
        "master[\"PE_ratio\"] = master[\"Close\"] / master[\"EPS\"]\n",
        "master[\"PE_ratio\"] = master[\"PE_ratio\"].where(master[\"EPS\"] > 0, other=np.nan)\n",
        "\n",
        "print(\"=== Master Panel ===\")\n",
        "print(\"Shape:\", master.shape)\n",
        "print(\"\\nColumns:\", master.columns.tolist())\n",
        "print(\"\\nMissing values:\")\n",
        "print(master.isna().sum())\n"
      ],
      "metadata": {
        "id": "PYZxGUUSNwzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "master.head()"
      ],
      "metadata": {
        "id": "DraHuJBoN98m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── 5d. Handle Missing Values ─────────────────────────────────────────\n",
        "\n",
        "# PE_ratio: NaN when EPS negative → forward fill per stock\n",
        "master[\"PE_ratio\"] = master.groupby(\"Stock\")[\"PE_ratio\"].ffill().bfill()\n",
        "\n",
        "# Drop rows where technical indicators are NaN\n",
        "# (first ~20 rows per stock due to rolling windows)\n",
        "master = master.dropna(subset=[\"LogReturn\",\"RSI14\",\"Volatility20\",\n",
        "                                \"Price_SMA20_ratio\",\"Target\"])\n",
        "\n",
        "print(\"Shape after cleaning:\", master.shape)\n",
        "print(\"\\nMissing values:\")\n",
        "print(master.isna().sum())"
      ],
      "metadata": {
        "id": "y5kyCsTxRlfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── Find days with zero or missing volume ─────────────────────────────\n",
        "zero_vol = master[master[\"LogVolume\"].isna()][[\"Date\",\"Stock\",\"Volume\",\"LogVolume\"]]\n",
        "print(\"Days with missing LogVolume:\")\n",
        "print(zero_vol[[\"Date\",\"Stock\",\"Volume\"]])"
      ],
      "metadata": {
        "id": "COj1IigVR1rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward fill LogVolume for holiday/zero volume days\n",
        "master[\"LogVolume\"] = master.groupby(\"Stock\")[\"LogVolume\"].ffill().bfill()\n",
        "\n",
        "# Verify\n",
        "print(\"Missing values after fix:\")\n",
        "print(master[[\"LogVolume\"]].isna().sum())\n",
        "print(\"\\nFinal master shape:\", master.shape)"
      ],
      "metadata": {
        "id": "vh4R0feQSEXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── 5e. Robust Scaling ────────────────────────────────────────────────\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Columns to scale — exclude identifiers and target\n",
        "cols_to_scale = [\"LogReturn\",\"Return_lag1\",\"RSI14\",\"Price_SMA20_ratio\",\n",
        "                 \"Volatility20\",\"LogVolume\",\"USDINR\",\"CrudeOil\",\n",
        "                 \"IN10Y\",\"CPI_YoY\",\"EPS\",\"ROE\",\"DebtEquity\",\n",
        "                 \"Sentiment\",\"PE_ratio\"]\n",
        "\n",
        "# ── 5f. Train / Test Split BEFORE scaling (no look-ahead bias) ────────\n",
        "train = master[master[\"Date\"] <  \"2025-10-01\"].copy()\n",
        "test  = master[master[\"Date\"] >= \"2025-10-01\"].copy()\n",
        "\n",
        "print(f\"Train: {train.shape} — {train['Date'].min().date()} to {train['Date'].max().date()}\")\n",
        "print(f\"Test:  {test.shape}  — {test['Date'].min().date()} to {test['Date'].max().date()}\")\n",
        "\n",
        "# ── Fit scaler on TRAIN only, transform both ──────────────────────────\n",
        "scaler = RobustScaler()\n",
        "train[cols_to_scale] = scaler.fit_transform(train[cols_to_scale])\n",
        "test[cols_to_scale]  = scaler.transform(test[cols_to_scale])\n",
        "\n",
        "print(\"\\n✅ Scaling done\")\n",
        "\n",
        "train[cols_to_scale].head()"
      ],
      "metadata": {
        "id": "usS1fF7oTMXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  6.MODEL TRAINING + VALIDATION"
      ],
      "metadata": {
        "id": "LpHyAEetT4Ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ── STEP 6: ML Model + Walk-Forward Validation ────────────────────────\n",
        "!pip install lightgbm -q\n",
        "\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import Ridge\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ── Feature and target columns ────────────────────────────────────────\n",
        "feature_cols = [\"LogReturn\",\"Return_lag1\",\"RSI14\",\"Price_SMA20_ratio\",\n",
        "                \"Volatility20\",\"LogVolume\",\"USDINR\",\"CrudeOil\",\n",
        "                \"IN10Y\",\"CPI_YoY\",\"EPS\",\"ROE\",\"DebtEquity\",\n",
        "                \"Sentiment\",\"PE_ratio\"]\n",
        "\n",
        "target_col = \"Target\"\n",
        "\n",
        "# ── 6a. Define Walk-Forward Folds ─────────────────────────────────────\n",
        "train[\"Date\"] = pd.to_datetime(train[\"Date\"])\n",
        "\n",
        "# Generate 6-month validation folds with minimum 12 months training\n",
        "folds      = []\n",
        "val_start  = pd.Timestamp(\"2021-01-01\")  # start after 12 months training\n",
        "fold_end   = pd.Timestamp(\"2025-09-30\")\n",
        "\n",
        "while val_start + pd.DateOffset(months=6) <= fold_end:\n",
        "    val_end = val_start + pd.DateOffset(months=6)\n",
        "    folds.append({\n",
        "        \"train_end\"  : val_start,\n",
        "        \"val_start\"  : val_start,\n",
        "        \"val_end\"    : val_end\n",
        "    })\n",
        "    val_start = val_end\n",
        "\n",
        "print(f\"Total folds: {len(folds)}\")\n",
        "for i, f in enumerate(folds):\n",
        "    print(f\"Fold {i+1}: Train → {f['train_end'].date()} | \"\n",
        "          f\"Val {f['val_start'].date()} → {f['val_end'].date()}\")"
      ],
      "metadata": {
        "id": "AfkB0YS4T6jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── 6b. RFE Feature Selection ─────────────────────────────────────────\n",
        "print(\"\\nRunning RFE to select best features...\")\n",
        "\n",
        "# Use Ridge regression for RFE (fast and stable)\n",
        "X_rfe = train[feature_cols].values\n",
        "y_rfe = train[target_col].values\n",
        "\n",
        "ridge   = Ridge(alpha=1.0)\n",
        "rfe     = RFE(estimator=ridge, n_features_to_select=10, step=1)\n",
        "rfe.fit(X_rfe, y_rfe)\n",
        "\n",
        "selected_features = [f for f, s in zip(feature_cols, rfe.support_) if s]\n",
        "dropped_features  = [f for f, s in zip(feature_cols, rfe.support_) if not s]\n",
        "\n",
        "print(f\"\\n✅ Selected features ({len(selected_features)}):\")\n",
        "print(selected_features)\n",
        "print(f\"\\n❌ Dropped features ({len(dropped_features)}):\")\n",
        "print(dropped_features)\n"
      ],
      "metadata": {
        "id": "7RSExTXhWFFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── 6c. Walk-Forward Training Loop ───────────────────────────────────\n",
        "fold_results = []\n",
        "\n",
        "lgb_params = {\n",
        "    \"objective\"        : \"regression\",\n",
        "    \"metric\"           : \"rmse\",\n",
        "    \"learning_rate\"    : 0.05,\n",
        "    \"num_leaves\"       : 31,\n",
        "    \"min_child_samples\": 20,\n",
        "    \"feature_fraction\" : 0.8,\n",
        "    \"bagging_fraction\" : 0.8,\n",
        "    \"bagging_freq\"     : 5,\n",
        "    \"reg_alpha\"        : 0.1,   # L1 regularization\n",
        "    \"reg_lambda\"       : 0.1,   # L2 regularization\n",
        "    \"verbose\"          : -1\n",
        "}\n",
        "\n",
        "for i, fold in enumerate(folds):\n",
        "    # ── Split data ────────────────────────────────────────────────────\n",
        "    fold_train = train[train[\"Date\"] <  fold[\"train_end\"]]\n",
        "    fold_val   = train[(train[\"Date\"] >= fold[\"val_start\"]) &\n",
        "                       (train[\"Date\"] <  fold[\"val_end\"])]\n",
        "\n",
        "    if len(fold_train) < 100 or len(fold_val) < 10:\n",
        "        continue\n",
        "\n",
        "    X_train = fold_train[selected_features].values\n",
        "    y_train = fold_train[target_col].values\n",
        "    X_val   = fold_val[selected_features].values\n",
        "    y_val   = fold_val[target_col].values\n",
        "\n",
        "    # ── Train LightGBM ────────────────────────────────────────────────\n",
        "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
        "    dval   = lgb.Dataset(X_val,   label=y_val, reference=dtrain)\n",
        "\n",
        "    model = lgb.train(\n",
        "        lgb_params,\n",
        "        dtrain,\n",
        "        num_boost_round      = 500,\n",
        "        valid_sets           = [dval],\n",
        "        callbacks            = [lgb.early_stopping(50, verbose=False),\n",
        "                                 lgb.log_evaluation(period=-1)]\n",
        "    )\n",
        "\n",
        "    # ── Predict and evaluate ──────────────────────────────────────────\n",
        "    y_pred = model.predict(X_val)\n",
        "\n",
        "    rmse    = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "    mae     = mean_absolute_error(y_val, y_pred)\n",
        "    dir_acc = np.mean(np.sign(y_pred) == np.sign(y_val)) * 100\n",
        "\n",
        "    # ── Per stock directional accuracy ────────────────────────────────\n",
        "    fold_val         = fold_val.copy()\n",
        "    fold_val[\"Pred\"] = y_pred\n",
        "    per_stock_acc    = fold_val.groupby(\"Stock\").apply(\n",
        "        lambda x: np.mean(np.sign(x[\"Pred\"]) == np.sign(x[\"Target\"])) * 100\n",
        "    ).round(2)\n",
        "\n",
        "    fold_results.append({\n",
        "        \"Fold\"      : i + 1,\n",
        "        \"Val_Start\" : fold[\"val_start\"].date(),\n",
        "        \"Val_End\"   : fold[\"val_end\"].date(),\n",
        "        \"RMSE\"      : round(rmse, 6),\n",
        "        \"MAE\"       : round(mae, 6),\n",
        "        \"DirAcc\"    : round(dir_acc, 2),\n",
        "        \"PerStock\"  : per_stock_acc\n",
        "    })\n",
        "\n",
        "    print(f\"Fold {i+1} | {fold['val_start'].date()} → {fold['val_end'].date()} | \"\n",
        "          f\"RMSE={rmse:.6f} | MAE={mae:.6f} | DirAcc={dir_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "mxPLKuiEXYM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── 6d. Summary of Walk-Forward Results ──────────────────────────────\n",
        "results_df = pd.DataFrame([{\n",
        "    \"Fold\"      : r[\"Fold\"],\n",
        "    \"Val_Start\" : r[\"Val_Start\"],\n",
        "    \"Val_End\"   : r[\"Val_End\"],\n",
        "    \"RMSE\"      : r[\"RMSE\"],\n",
        "    \"MAE\"       : r[\"MAE\"],\n",
        "    \"DirAcc\"    : r[\"DirAcc\"]\n",
        "} for r in fold_results])\n",
        "\n",
        "print(\"\\n=== Walk-Forward Validation Results ===\")\n",
        "print(results_df.to_string(index=False))\n",
        "print(f\"\\nAverage RMSE : {results_df['RMSE'].mean():.6f}\")\n",
        "print(f\"Average MAE  : {results_df['MAE'].mean():.6f}\")\n",
        "print(f\"Average DirAcc: {results_df['DirAcc'].mean():.2f}%\")\n",
        "\n",
        "# ── Per stock average directional accuracy ────────────────────────────\n",
        "print(\"\\n=== Per Stock Directional Accuracy (avg across folds) ===\")\n",
        "all_stock_acc = pd.concat([r[\"PerStock\"] for r in fold_results], axis=1)\n",
        "all_stock_acc.columns = [f\"Fold{r['Fold']}\" for r in fold_results]\n",
        "all_stock_acc[\"Average\"] = all_stock_acc.mean(axis=1).round(2)\n",
        "print(all_stock_acc)"
      ],
      "metadata": {
        "id": "qWHRqp2FX3Au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── 6e. Train Final Model on Full Training Data ───────────────────────\n",
        "print(\"\\nTraining final model on full training data...\")\n",
        "\n",
        "X_train_full = train[selected_features].values\n",
        "y_train_full = train[target_col].values\n",
        "\n",
        "dtrain_full = lgb.Dataset(X_train_full, label=y_train_full)\n",
        "\n",
        "final_model = lgb.train(\n",
        "    lgb_params,\n",
        "    dtrain_full,\n",
        "    num_boost_round = 500,\n",
        "    callbacks       = [lgb.log_evaluation(period=-1)]\n",
        ")\n",
        "\n",
        "# ── Predict on test set (Oct-Dec 2025) ────────────────────────────────\n",
        "X_test  = test[selected_features].values\n",
        "y_test  = test[target_col].values\n",
        "y_test_pred = final_model.predict(X_test)\n",
        "\n",
        "test = test.copy()\n",
        "test[\"Predicted_Return\"] = y_test_pred\n",
        "\n",
        "# ── Test set metrics ──────────────────────────────────────────────────\n",
        "test_rmse    = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "test_mae     = mean_absolute_error(y_test, y_test_pred)\n",
        "test_dir_acc = np.mean(np.sign(y_test_pred) == np.sign(y_test)) * 100\n",
        "\n",
        "print(\"\\n=== Final Test Set Results (Oct-Dec 2025) ===\")\n",
        "print(f\"RMSE            : {test_rmse:.6f}\")\n",
        "print(f\"MAE             : {test_mae:.6f}\")\n",
        "print(f\"Directional Acc : {test_dir_acc:.2f}%\")\n",
        "\n",
        "print(\"\\n=== Per Stock Test Accuracy ===\")\n",
        "test_stock_acc = test.groupby(\"Stock\").apply(\n",
        "    lambda x: np.mean(np.sign(x[\"Predicted_Return\"]) == np.sign(x[\"Target\"])) * 100\n",
        ").round(2)\n",
        "print(test_stock_acc)"
      ],
      "metadata": {
        "id": "YswVWHcTYQBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 7. PORTFOLIO"
      ],
      "metadata": {
        "id": "JcC7GzJCY2bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ── STEP 7: Complete Portfolio Construction ───────────────────────────\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import yfinance as yf\n",
        "\n",
        "# ── 7a. Pivot to wide format ──────────────────────────────────────────\n",
        "pred_wide   = test.pivot(index=\"Date\", columns=\"Stock\",\n",
        "                          values=\"Predicted_Return\")\n",
        "actual_wide = test.pivot(index=\"Date\", columns=\"Stock\",\n",
        "                          values=\"Target\")\n",
        "\n",
        "stocks_in_test = actual_wide.columns.tolist()\n",
        "print(\"Stocks in portfolio:\", stocks_in_test)\n",
        "print(\"Trading days:\", len(pred_wide))\n",
        "\n",
        "# ── 7b. Dynamic weights from predicted returns ────────────────────────\n",
        "pred_clipped  = pred_wide.clip(lower=0)\n",
        "daily_weights = pred_clipped.div(\n",
        "    pred_clipped.sum(axis=1).replace(0, np.nan), axis=0\n",
        ").fillna(1/6)\n",
        "\n",
        "print(\"\\n=== Sample Daily Weights ===\")\n",
        "print(daily_weights.head(5).round(4))\n",
        "print(\"\\nWeight sum per day (should all be 1.0):\")\n",
        "print(daily_weights.sum(axis=1).head(5).round(4))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-653MVpc935O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── 7c. Portfolio returns ─────────────────────────────────────────────\n",
        "portfolio_returns = (actual_wide * daily_weights).sum(axis=1)\n",
        "\n",
        "print(\"\\n=== Portfolio Daily Returns ===\")\n",
        "print(portfolio_returns.head(10))\n",
        "print(f\"\\nMean daily return : {portfolio_returns.mean():.6f}\")\n",
        "print(f\"Std daily return  : {portfolio_returns.std():.6f}\")\n",
        "\n",
        "# ── 7d. Performance metrics ───────────────────────────────────────────\n",
        "trading_days   = 252\n",
        "risk_free_rate = 0.065 / trading_days\n",
        "\n",
        "# Sharpe Ratio\n",
        "sharpe = ((portfolio_returns.mean() - risk_free_rate) /\n",
        "           portfolio_returns.std()) * np.sqrt(trading_days)\n",
        "\n",
        "# Equity curve\n",
        "cum_returns = (1 + portfolio_returns).cumprod()\n",
        "\n",
        "# Maximum Drawdown\n",
        "rolling_max = cum_returns.cummax()\n",
        "drawdown    = (cum_returns - rolling_max) / rolling_max\n",
        "mdd         = drawdown.min()\n",
        "\n",
        "# Hit Ratio\n",
        "hit_ratio = (portfolio_returns > 0).mean() * 100\n",
        "\n",
        "# Annualized metrics\n",
        "ann_return = portfolio_returns.mean() * trading_days * 100\n",
        "ann_vol    = portfolio_returns.std() * np.sqrt(trading_days) * 100\n",
        "\n",
        "print(\"\\n=== Portfolio Performance Metrics ===\")\n",
        "print(f\"Annualized Return     : {ann_return:.2f}%\")\n",
        "print(f\"Annualized Volatility : {ann_vol:.2f}%\")\n",
        "print(f\"Sharpe Ratio          : {sharpe:.4f}\")\n",
        "print(f\"Maximum Drawdown      : {mdd*100:.2f}%\")\n",
        "print(f\"Hit Ratio             : {hit_ratio:.2f}%\")"
      ],
      "metadata": {
        "id": "FEiLmdVl_kMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── Accuracy-Based Weights (HUL = 0) ─────────────────────────────────\n",
        "test_accuracy = test.groupby(\"Stock\").apply(\n",
        "    lambda x: np.mean(np.sign(x[\"Predicted_Return\"]) == np.sign(x[\"Target\"])) * 100\n",
        ").sort_values(ascending=False)\n",
        "\n",
        "print(\"=== Per Stock Test Accuracy ===\")\n",
        "print(test_accuracy)\n",
        "\n",
        "# ── Assign weights in order of accuracy, HUL = 0 ─────────────────────\n",
        "custom_acc_weights = pd.Series({\n",
        "    \"BHARTIARTL.NS\" : 0.30,   # 60.00% — best\n",
        "    \"M&M.NS\"        : 0.25,   # 55.00% — second\n",
        "    \"RELIANCE.NS\"   : 0.20,   # 53.33% — third\n",
        "    \"HDFCBANK.NS\"   : 0.13,   # 51.67% — fourth\n",
        "    \"INFY.NS\"       : 0.12,   # 51.67% — fifth\n",
        "    \"HINDUNILVR.NS\" : 0.00,   # 36.67% — zero\n",
        "})\n",
        "\n",
        "print(\"\\n=== Accuracy-Based Weights (HUL=0) ===\")\n",
        "for stock, w in custom_acc_weights.items():\n",
        "    acc = test_accuracy[stock]\n",
        "    print(f\"  {stock:20s} → {w*100:.0f}%  (accuracy: {acc:.2f}%)\")\n",
        "print(f\"\\nTotal: {custom_acc_weights.sum():.2f}\")\n",
        "\n",
        "# ── Portfolio returns ─────────────────────────────────────────────────\n",
        "w_acc = custom_acc_weights[stocks_in_test]\n",
        "w_acc = w_acc / w_acc.sum()  # renormalize to 1\n",
        "\n",
        "portfolio_returns_acc = actual_wide.dot(w_acc)\n",
        "\n",
        "# ── Metrics ───────────────────────────────────────────────────────────\n",
        "sharpe_acc  = ((portfolio_returns_acc.mean() - risk_free_rate) /\n",
        "                portfolio_returns_acc.std()) * np.sqrt(trading_days)\n",
        "cum_acc     = (1 + portfolio_returns_acc).cumprod()\n",
        "mdd_acc     = ((cum_acc - cum_acc.cummax()) / cum_acc.cummax()).min()\n",
        "hit_acc     = (portfolio_returns_acc > 0).mean() * 100\n",
        "ann_ret_acc = portfolio_returns_acc.mean() * trading_days * 100\n",
        "\n",
        "print(\"\\n=== Accuracy-Based Portfolio Metrics ===\")\n",
        "print(f\"Annualized Return     : {ann_ret_acc:.2f}%\")\n",
        "print(f\"Sharpe Ratio          : {sharpe_acc:.4f}\")\n",
        "print(f\"Max Drawdown          : {mdd_acc*100:.2f}%\")\n",
        "print(f\"Hit Ratio             : {hit_acc:.2f}%\")\n",
        "\n",
        "# ── Compare both methods ──────────────────────────────────────────────\n",
        "print(\"\\n=== Dynamic vs Accuracy-Based ===\")\n",
        "print(f\"{'Metric':<25} {'Dynamic':>12} {'Acc-Based':>12}\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"{'Annualized Return':<25} {ann_return:>11.2f}% {ann_ret_acc:>11.2f}%\")\n",
        "print(f\"{'Sharpe Ratio':<25} {sharpe:>12.4f} {sharpe_acc:>12.4f}\")\n",
        "print(f\"{'Max Drawdown':<25} {mdd*100:>11.2f}% {mdd_acc*100:>11.2f}%\")\n",
        "print(f\"{'Hit Ratio':<25} {hit_ratio:>11.2f}% {hit_acc:>11.2f}%\")"
      ],
      "metadata": {
        "id": "eQ0so3-0Cpn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── Static Portfolio — Average Predicted Return ───────────────────────\n",
        "\n",
        "# Step 1: Calculate average predicted return per stock over test period\n",
        "avg_predicted = pred_wide.mean()\n",
        "print(\"=== Average Predicted Return per Stock ===\")\n",
        "print(avg_predicted.sort_values(ascending=False))\n",
        "\n",
        "# Step 2: Clip negative predictions to 0\n",
        "avg_predicted_clipped = avg_predicted.clip(lower=0)\n",
        "\n",
        "# Step 3: Normalize to sum to 1\n",
        "static_weights = avg_predicted_clipped / avg_predicted_clipped.sum()\n",
        "\n",
        "print(\"\\n=== Static Weights (from avg predicted return) ===\")\n",
        "for stock, w in static_weights.sort_values(ascending=False).items():\n",
        "    print(f\"  {stock:20s} → {w*100:.2f}%\")\n",
        "print(f\"\\nTotal: {static_weights.sum():.4f}\")\n",
        "\n",
        "# Step 4: Apply fixed weights to actual returns every day\n",
        "portfolio_returns_static = actual_wide.dot(static_weights)\n",
        "\n",
        "# Step 5: Metrics\n",
        "sharpe_static  = ((portfolio_returns_static.mean() - risk_free_rate) /\n",
        "                   portfolio_returns_static.std()) * np.sqrt(252)\n",
        "cum_static     = (1 + portfolio_returns_static).cumprod()\n",
        "mdd_static     = ((cum_static - cum_static.cummax()) /\n",
        "                   cum_static.cummax()).min()\n",
        "hit_static     = (portfolio_returns_static > 0).mean() * 100\n",
        "ann_ret_static = portfolio_returns_static.mean() * 252 * 100\n",
        "\n",
        "print(\"\\n=== Static Portfolio Metrics ===\")\n",
        "print(f\"Annualized Return : {ann_ret_static:.2f}%\")\n",
        "print(f\"Sharpe Ratio      : {sharpe_static:.4f}\")\n",
        "print(f\"Max Drawdown      : {mdd_static*100:.2f}%\")\n",
        "print(f\"Hit Ratio         : {hit_static:.2f}%\")\n",
        "print(f\"Ending Value      : ₹{cum_static.iloc[-1]:.4f}\")\n"
      ],
      "metadata": {
        "id": "XghgS_ZpJiBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── Static approach-2 (Hybrid)───────────────────────────────────────────────────\n",
        "avg_predicted = pred_wide.mean()\n",
        "\n",
        "# Zero out stocks with accuracy below 50%\n",
        "bad_stocks = [\"HINDUNILVR.NS\"]  # accuracy 36.67%\n",
        "avg_predicted[bad_stocks] = 0\n",
        "\n",
        "# Clip remaining negatives and normalize\n",
        "avg_clipped = avg_predicted.clip(lower=0)\n",
        "hybrid_weights = avg_clipped / avg_clipped.sum()\n",
        "\n",
        "print(\"=== Hybrid Weights ===\")\n",
        "for stock, w in hybrid_weights.sort_values(ascending=False).items():\n",
        "    print(f\"  {stock:20s} → {w*100:.2f}%\")\n",
        "\n",
        "# Portfolio returns\n",
        "portfolio_returns_hybrid = actual_wide.dot(hybrid_weights)\n",
        "\n",
        "# Metrics\n",
        "sharpe_hybrid  = ((portfolio_returns_hybrid.mean() - risk_free_rate) /\n",
        "                   portfolio_returns_hybrid.std()) * np.sqrt(252)\n",
        "cum_hybrid     = (1 + portfolio_returns_hybrid).cumprod()\n",
        "mdd_hybrid     = ((cum_hybrid - cum_hybrid.cummax()) /\n",
        "                   cum_hybrid.cummax()).min()\n",
        "hit_hybrid     = (portfolio_returns_hybrid > 0).mean() * 100\n",
        "ann_ret_hybrid = portfolio_returns_hybrid.mean() * 252 * 100\n",
        "\n",
        "print(\"\\n=== Hybrid Portfolio Metrics ===\")\n",
        "print(f\"Annualized Return : {ann_ret_hybrid:.2f}%\")\n",
        "print(f\"Sharpe Ratio      : {sharpe_hybrid:.4f}\")\n",
        "print(f\"Max Drawdown      : {mdd_hybrid*100:.2f}%\")\n",
        "print(f\"Hit Ratio         : {hit_hybrid:.2f}%\")"
      ],
      "metadata": {
        "id": "EeYF0KH2J2ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── 7e. Benchmark — Nifty50 ──────────────────────────────────────────\n",
        "nifty_test = yf.download(\"^NSEI\", start=\"2025-10-01\",\n",
        "                          end=\"2025-12-31\", auto_adjust=True)\n",
        "nifty_test.columns = nifty_test.columns.get_level_values(0)\n",
        "nifty_ret  = nifty_test[\"Close\"].pct_change().dropna()\n",
        "nifty_ret.index = pd.to_datetime(nifty_ret.index)\n",
        "\n",
        "# Align dates\n",
        "common_dates  = portfolio_returns.index.intersection(nifty_ret.index)\n",
        "port_aligned  = portfolio_returns.loc[common_dates]\n",
        "nifty_aligned = nifty_ret.loc[common_dates]\n",
        "\n",
        "# Nifty metrics\n",
        "nifty_sharpe  = ((nifty_aligned.mean() - risk_free_rate) /\n",
        "                  nifty_aligned.std()) * np.sqrt(trading_days)\n",
        "nifty_cum     = (1 + nifty_aligned).cumprod()\n",
        "nifty_mdd     = ((nifty_cum - nifty_cum.cummax()) /\n",
        "                  nifty_cum.cummax()).min()\n",
        "nifty_hit     = (nifty_aligned > 0).mean() * 100\n",
        "nifty_ann_ret = nifty_aligned.mean() * trading_days * 100\n",
        "\n",
        "# ── Full comparison including Accuracy-Based ──────────────────────────\n",
        "print(\"\\n=== All Methods Comparison ===\")\n",
        "print(f\"{'Metric':<25} {'Dynamic':>10} {'Static':>10} {'Acc-Based':>10} {'Hybrid':>10} {'Nifty50':>10}\")\n",
        "print(\"-\" * 78)\n",
        "print(f\"{'Annualized Return':<25} {ann_return:>9.2f}% {ann_ret_static:>9.2f}% {ann_ret_acc:>9.2f}% {ann_ret_hybrid:>9.2f}% {nifty_ann_ret:>9.2f}%\")\n",
        "print(f\"{'Sharpe Ratio':<25} {sharpe:>10.4f} {sharpe_static:>10.4f} {sharpe_acc:>10.4f} {sharpe_hybrid:>10.4f} {nifty_sharpe:>10.4f}\")\n",
        "print(f\"{'Max Drawdown':<25} {mdd*100:>9.2f}% {mdd_static*100:>9.2f}% {mdd_acc*100:>9.2f}% {mdd_hybrid*100:>9.2f}% {nifty_mdd*100:>9.2f}%\")\n",
        "print(f\"{'Hit Ratio':<25} {hit_ratio:>9.2f}% {hit_static:>9.2f}% {hit_acc:>9.2f}% {hit_hybrid:>9.2f}% {nifty_hit:>9.2f}%\")"
      ],
      "metadata": {
        "id": "82PfHoB5J_8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Going ahead with dynamic portfolio ratio allocation"
      ],
      "metadata": {
        "id": "fekNp5cuCxHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing to NIFTY"
      ],
      "metadata": {
        "id": "3tYHUefMC8jC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Benchmark Comparison ===\")\n",
        "print(f\"{'Metric':<25} {'Portfolio':>12} {'Nifty50':>12}\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"{'Annualized Return':<25} {ann_return:>11.2f}% {nifty_ann_ret:>11.2f}%\")\n",
        "print(f\"{'Sharpe Ratio':<25} {sharpe:>12.4f} {nifty_sharpe:>12.4f}\")\n",
        "print(f\"{'Max Drawdown':<25} {mdd*100:>11.2f}% {nifty_mdd*100:>11.2f}%\")\n",
        "print(f\"{'Hit Ratio':<25} {hit_ratio:>11.2f}% {nifty_hit:>11.2f}%\")\n",
        "\n",
        "# ── 7f. Equity Curve ─────────────────────────────────────────────────\n",
        "port_cum_aligned  = (1 + port_aligned).cumprod()\n",
        "nifty_cum_aligned = (1 + nifty_aligned).cumprod()\n",
        "drawdown_aligned  = (port_cum_aligned -\n",
        "                     port_cum_aligned.cummax()) / port_cum_aligned.cummax()\n",
        "\n",
        "fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
        "fig.suptitle(\"Portfolio Performance: Oct - Dec 2025\",\n",
        "             fontsize=16, fontweight=\"bold\")\n",
        "\n",
        "# Plot 1: Equity Curve\n",
        "axes[0].plot(port_cum_aligned.index, port_cum_aligned.values,\n",
        "             color=\"blue\", linewidth=2, label=\"ML Portfolio\")\n",
        "axes[0].plot(nifty_cum_aligned.index, nifty_cum_aligned.values,\n",
        "             color=\"orange\", linewidth=2, linestyle=\"--\", label=\"Nifty50\")\n",
        "axes[0].fill_between(port_cum_aligned.index,\n",
        "                     port_cum_aligned.values,\n",
        "                     nifty_cum_aligned.values,\n",
        "                     where=port_cum_aligned.values >= nifty_cum_aligned.values,\n",
        "                     alpha=0.2, color=\"green\", label=\"Outperforming\")\n",
        "axes[0].fill_between(port_cum_aligned.index,\n",
        "                     port_cum_aligned.values,\n",
        "                     nifty_cum_aligned.values,\n",
        "                     where=port_cum_aligned.values < nifty_cum_aligned.values,\n",
        "                     alpha=0.2, color=\"red\", label=\"Underperforming\")\n",
        "axes[0].set_title(\"Equity Curve (₹1 invested at start)\", fontsize=13)\n",
        "axes[0].set_ylabel(\"Portfolio Value (₹)\")\n",
        "axes[0].legend(loc=\"upper left\")\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].xaxis.set_major_formatter(mdates.DateFormatter(\"%b %Y\"))\n",
        "\n",
        "# Plot 2: Drawdown\n",
        "axes[1].fill_between(drawdown_aligned.index,\n",
        "                     drawdown_aligned.values * 100,\n",
        "                     0, color=\"red\", alpha=0.5)\n",
        "axes[1].plot(drawdown_aligned.index,\n",
        "             drawdown_aligned.values * 100,\n",
        "             color=\"darkred\", linewidth=1)\n",
        "axes[1].axhline(y=mdd * 100, color=\"black\",\n",
        "                linestyle=\"--\", linewidth=0.8,\n",
        "                label=f\"Max DD: {mdd*100:.2f}%\")\n",
        "axes[1].set_title(\"Portfolio Drawdown\", fontsize=13)\n",
        "axes[1].set_ylabel(\"Drawdown (%)\")\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].xaxis.set_major_formatter(mdates.DateFormatter(\"%b %Y\"))\n",
        "\n",
        "# Plot 3: Daily Returns\n",
        "colors_bar = [\"#2ecc71\" if r > 0 else \"#e74c3c\"\n",
        "              for r in port_aligned.values]\n",
        "axes[2].bar(port_aligned.index, port_aligned.values * 100,\n",
        "            color=colors_bar, alpha=0.8, width=1)\n",
        "axes[2].axhline(y=0, color=\"black\", linewidth=0.8)\n",
        "axes[2].axhline(y=port_aligned.mean() * 100,\n",
        "                color=\"blue\", linestyle=\"--\", linewidth=1,\n",
        "                label=f\"Mean: {port_aligned.mean()*100:.3f}%\")\n",
        "axes[2].set_title(\"Daily Portfolio Returns\", fontsize=13)\n",
        "axes[2].set_ylabel(\"Daily Return (%)\")\n",
        "axes[2].legend()\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "axes[2].xaxis.set_major_formatter(mdates.DateFormatter(\"%b %Y\"))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"equity_curve.png\", dpi=150, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "print(\"Equity curve saved!\")\n",
        "\n",
        "# ── Final Summary ───────────────────────────────────────────────────────────\n",
        "print(f\"\\n=== Final Summary ===\")\n",
        "print(f\"Starting Value  : ₹1.0000\")\n",
        "print(f\"Ending Value    : ₹{port_cum_aligned.iloc[-1]:.4f}\")\n",
        "print(f\"Total Return    : {(port_cum_aligned.iloc[-1]-1)*100:.2f}%\")\n",
        "print(f\"Nifty Return    : {(nifty_cum_aligned.iloc[-1]-1)*100:.2f}%\")\n",
        "print(f\"Alpha vs Nifty  : {((port_cum_aligned.iloc[-1]-nifty_cum_aligned.iloc[-1])*100):.2f}%\")"
      ],
      "metadata": {
        "id": "2GOBsZcQC7he"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── Feature Importance ────────────────────────────────────────────────\n",
        "importance = final_model.feature_importance(importance_type=\"gain\")\n",
        "importance_df = pd.DataFrame({\n",
        "    \"Feature\"   : selected_features,\n",
        "    \"Importance\": importance\n",
        "}).sort_values(\"Importance\", ascending=True)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(importance_df[\"Feature\"], importance_df[\"Importance\"], color=\"steelblue\")\n",
        "plt.title(\"Feature Importance (LightGBM — Gain)\", fontsize=14)\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"feature_importance.png\", dpi=150, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "print(\"✅ Saved as feature_importance.png\")"
      ],
      "metadata": {
        "id": "NtVvawyMJZdZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}